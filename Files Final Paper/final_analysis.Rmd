---
title: "Data Analysis for Give me Gestalt - Cubist Art Experiment"
Authors: "Group 03"
---

# Instructions

## Required R packages
* `tidyverse` (or `ggplot2`, `dplyr`, `purrr`, `tibble`)
* `brms`
* `devtools` (for installing `faintr`)
* `rstan`
* `faintr` (install with `devtools`)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)

```

Load the required packages and set a seed for the random number generator.
```{r}
library(tidyverse)

library(rstan)
# set cores to use to the total number of cores (minimally 4)
options(mc.cores = max(parallel::detectCores(), 4))
# save a compiled version of the Stan model file
rstan_options(auto_write = TRUE)

library(brms)

# install faintr with devtools::install_github('michael-franke/bayes_mixed_regression_tutorial/faintr', build_vignettes = TRUE))
library(faintr)

set.seed(123)

```

Step 1. Read the data and take a glimpse of it.
---

```{r}
data_raw <- read_csv("results_71_givemegestalt_group3_group3.csv")
glimpse(data_raw)
```


Step 2. Select only the columns of interest and filter trials that are not interesting for the calculations.
Change data type of the responses to double and submission_id to factor.
---

```{r}
data_selection <- select(data_raw, submission_id, snellen_result, is_expert, trial_name, trial_number, fileID, response, RT, timeSpent)
data_filtered <- filter(data_selection, trial_name != "test_snellen", trial_name != "b0_expert")
data_mut <- mutate(data_filtered, response = as.double(response), submission_id = factor(submission_id))

show(data_selection)
show(data_filtered)
show(data_mut)

```


Step 3. Split data according to trial (b1: Liking, b2: Detactability) and special features.
---

```{r}
data_b1 <- filter(data_mut, trial_name == "b1_exp")
data_b2 <- filter(data_mut, trial_name == "b2_exp")

data_experts <- filter(data_mut, is_expert == "Yes")
data_snellen_failed <- filter(data_mut, snellen_result < 7)

show(data_b1)
show(data_b2)
show(data_experts) # why is one missing?
show(data_snellen_failed)
```

Step 4. Average data.
        Generate table with needed non-aggregated data in useful format.
        
---

```{r}
# Averaged data:
data_b1_mean <- group_by(data_b1, fileID) %>% summarise(mean_Like = mean(response))
data_b2_mean <- group_by(data_b2, fileID) %>% summarise(mean_Detect = mean(response))

show(data_b1_mean)
show(data_b2_mean)

final_data_mean <- merge(x = data_b2_mean, y = data_b1_mean, by = "fileID")
show(final_data_mean)

# Non-average data:
data_b1_mutated <- mutate(data_b1, liking = response, RT_liking = RT)
data_b2_mutated <- mutate(data_b2, detectability = response, RT_detectability = RT)

data_b1_selected <- select(data_b1_mutated, fileID, submission_id, snellen_result, timeSpent, liking, RT_liking)
data_b2_selected <- select(data_b2_mutated, fileID, submission_id, snellen_result, timeSpent, detectability, RT_detectability)

data_b1_orderd <- data_b1_selected[order(data_b1_selected$fileID),]
data_b2_orderd <- data_b2_selected[order(data_b2_selected$fileID),]

merged_data <- data_b1_orderd
merged_data$detectability <- data_b2_orderd$detectability
merged_data$RT_detectability <- data_b2_orderd$RT_detectability

show(data_b1_selected)
show(data_b2_selected)
show(data_b1_orderd)
show(data_b2_orderd)
show(merged_data)

final_data <- as_tibble(merged_data)
show(final_data)
```

Step 5.Test assumptions for linear regression.
---
Normality:
```{r}
# Non-averaged data:
ks.test(data_b1$response, pnorm)
ks.test(data_b2$response, pnorm)

# Averaged data:
ks.test(data_b1_mean$mean_Like, pnorm)
ks.test(data_b2_mean$mean_Detect, pnorm)
```

Result: According to the Kolmogorov-Smirnov test the variables are not normal distributed. Hence, the normality assumption is not fullfiled. We will test the residual distribution in the next step to make sure that we can still use a linear regression.

Further assumptions:
```{r}
# Non-averaged data:
hist(final_data$liking)
hist(final_data$detectability)
par(mfrow=c(2,2))
typeof(final_data)

mod <- lm(final_data$detectability ~ final_data$liking, data=final_data) #linear regression model
gvlma::gvlma(mod) # further tests for assumtions

summary(mod) # Show summary to check p-value
plot(resid(mod)) # Plot residuals to check if normality assumption is necessary
```
Result: For the non-averaged data, most of the assumptions necessary for linear-regression are not satisfied. Eventhough the violation of the normality assumtion is not important due to the huge sample size and the destribution of the residuals (https://iovs.arvojournals.org/article.aspx?articleid=2128171), we cannot use a linear regression for this data. 
We decided to use the aggregated data. 
```{r}
# Averaged data:
hist(data_b1_mean$mean_Like)
hist(data_b2_mean$mean_Detect)
par(mfrow=c(2,2))
typeof(data_b1_mean)

mod_mean <- lm( data_b2_mean$mean_Detect ~ data_b1_mean$mean_Like, data=final_data_mean) # linear regression model
gvlma::gvlma(mod_mean) # further tests for assumtions

summary(mod_mean) # Show summary to check p-value
plot(resid(mod_mean)) # Plot residuals to check if normality assumption is necessary
```
Result: Most of the assumptions for linear regression are satisfied. The distribution of the residuals shows asymmetry along the zero line, but not enough to violate the normality assumption. The p-value shows us that our model is significant. The R-squared value is lower than in the original paper.

Regression: y = -1.3428 + 1.4901 * x

Step 6. Make a plot of the averaged data.
---

```{r}
cor(data_b1_mean$mean_Like , data_b2_mean$mean_Detect)
scatter.smooth(y=data_b1_mean$mean_Like, x= data_b2_mean$mean_Detect, xlab = "Means Detection Scores", ylab = "Means Liking Scores")
ggplot(final_data_mean, aes(x = mean_Detect, y = mean_Like)) + geom_point() + geom_smooth(method = "lm")
```
Results: The correlation between the two variables Liking and Detectability is positive, but slightly lower than in the original experiment.
The lower R-squared value can be explained with two outliers.

Step 7. Run bayesian fixed-effects models
---

```{r}

brm_data_gaussian <- brm(formula=mean_Detect ~ mean_Like, data=final_data_mean, family = "gaussian")
summary(brm_data_gaussian)
```

Cumulative family for mixed/random effects:
```{r}
brm_mixed <- brm(formula=liking ~ mo(detectability) + (1|submission_id), data=final_data_mutated, family = "cumulative")
summary(brm_mixed)
```

```{r}
brm_mixed2 <- brm(formula=liking ~ mo(detectability) + (1|submission_id) + (1|fileID), data=final_data_mutated, family = "cumulative")
summary(brm_mixed2)
```

Step 8. Compare different bmr models.
---
```{r}

loo(brm_data_cumulative, brm_mixed)

```

```{r}

loo(brm_data_cumulative, brm_mixed2)

```

```{r}

loo(brm_mixed, brm_mixed2)

```

Plotting:
```{r}

plot(brm_data_gaussian)
```

```{r}
marginal_effects(brm_mixed)

```
